{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di plexagonchallenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmg6CbLINQQ4"
      },
      "source": [
        "**CHALLENGE OVERVIEW**\n",
        "\n",
        "I try different techniques as suggested in the provided links:\n",
        "\n",
        "1.   REDUCE MODEL SIZE\n",
        "2.   PRUNING\n",
        "3.   QUANTIZATION\n",
        "4.   CLUSTERING\n",
        "5.   DISTILLATION\n",
        "\n",
        "**PHASE 1 - BASELINE** <br>\n",
        "I run the baseline training (the model as it is delivered), to be able to optimize and compare it.\n",
        "\n",
        "**PHASE 2 - REDUCE MODEL** <br>\n",
        "I decrease the size of every layer in the model to reduce the number of parameters and make the model faster (most naive approach). <br>\n",
        "n.b. I also try distillation to train a lighter model, but as I notice a better performance with the simple reduction I choose not to use Distillation.\n",
        "\n",
        "**PHASE 3 - OPTIMIZATION** <br>\n",
        "I experiment mentioned optimizations on given model by using them singularly or combining them. I then plot the results to find the best tradeoff among 3 criteria:\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Time (Lower is better)\n",
        "*   Model weight (Lower is better)\n",
        "\n",
        "The reason is that I want to keep accuracy high while decreasing Time and Weight as much as possible as requested by the challange.\n",
        "\n",
        "Using the Reduced model and applying Quantization and then Pruning seems the best solution.\n",
        "This is because Redux operation hugely decreases model dimension. Also doing pruning as the last operations seems more beneficial, rather than using it before quantization.\n",
        "\n",
        "I choose the rqp model and save it as .h5 file.\n",
        "\n",
        "**PHASE 4 - COREML** <br>\n",
        "The third request is to convert the model to a supported format, preferably CoreML.\n",
        "I use the appropriate library to convert the model to a CoreML format to be easily used in an IOS environment.\n",
        "\n",
        "**EXTRA** <br>\n",
        "The \"run_model.py\" program is provided to classify a single image using the CoreML model in a MacOS environment. <br>\n",
        "Command: <br>\n",
        "python run_model.py --image *image_name* <br>\n",
        "--image default option is 'fashion1.png'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3XiYFRNk5Vh"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIfh8hd5ljrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558e5879-5c06-4322-d083-a5854305e873"
      },
      "source": [
        "#Load the dataset.\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbmJQ_EulozQ"
      },
      "source": [
        "#Add a trailing unitary dimension to make a 3D multidimensional array (tensor).\n",
        "# N x 28 x 28 --> N x 28 x 28 x 1\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRMMx-SNlr1t",
        "outputId": "8023e1eb-7d29-4ef1-ea97-0b0fd5173e56"
      },
      "source": [
        "#Convert the labels from integers to one-hot encoding.\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTGqkXWjlu71"
      },
      "source": [
        "LR = 1E-3 \n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQQFM1KClxjy"
      },
      "source": [
        "# Define baseline (and redux) models and train/test functions\n",
        "def build_model(input_shape):\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
        "    model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='elu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
        "    model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='elu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
        "    model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(256))\n",
        "    model.add(tf.keras.layers.Activation('elu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(10))\n",
        "    model.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Reduced model using halved dimensions\n",
        "def build_model_reduced(input_shape):\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
        "    model.add(tf.keras.layers.Conv2D(32, (5, 5), padding='same', activation='elu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
        "    model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='elu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
        "    model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='elu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(128))\n",
        "    model.add(tf.keras.layers.Activation('elu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(10))\n",
        "    model.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def train(x_train, y_train, x_test, y_test, redux = False):\n",
        "    \"\"\"\n",
        "    Train the model given the dataset and the global parameters (LR, EPOCHS and BATCH_SIZE).\n",
        "\n",
        "    The model is automalically saved after the training.\n",
        "\n",
        "    \"\"\"\n",
        "    if redux:\n",
        "      model = build_model_reduced(x_train.shape[1:])\n",
        "    else:\n",
        "      model = build_model(x_train.shape[1:])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['categorical_accuracy'],\n",
        "    )\n",
        "    print(model.summary())\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.fit(\n",
        "        x=x_train.astype(np.float32),\n",
        "        y=y_train.astype(np.float32),\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)),\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(\"Train elapsed time: {} seconds\".format(end_time - start_time))\n",
        "\n",
        "    if redux:\n",
        "      model.save(\"fashion_mnist_model_redux.tf\", overwrite=True)\n",
        "    else:\n",
        "      model.save(\"fashion_mnist_model.tf\", overwrite=True)\n",
        "    \n",
        "\n",
        "\n",
        "def test(x_test, y_test, redux = False):\n",
        "    \"\"\"\n",
        "    Load the saved model and evaluate it against the test set.\n",
        "    \"\"\"\n",
        "    if redux:\n",
        "      model = tf.keras.models.load_model(\"./fashion_mnist_model_redux.tf\")\n",
        "    else:\n",
        "      model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.evaluate(x_test, y_test)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(\"Test elapsed time: {} seconds\".format(end_time - start_time))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjJHdck_l2CR",
        "outputId": "18e6e689-39b5-42df-f5d5-af5036576a04"
      },
      "source": [
        "# BASELINE\n",
        "train(x_train, y_train, x_test, y_test)\n",
        "test(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,470\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 386\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "938/938 [==============================] - 43s 12ms/step - loss: 0.6561 - categorical_accuracy: 0.7861 - val_loss: 0.3555 - val_categorical_accuracy: 0.8688\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 10s 11ms/step - loss: 0.3831 - categorical_accuracy: 0.8616 - val_loss: 0.3251 - val_categorical_accuracy: 0.8835\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 10s 10ms/step - loss: 0.3297 - categorical_accuracy: 0.8806 - val_loss: 0.3095 - val_categorical_accuracy: 0.8922\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 10s 10ms/step - loss: 0.2997 - categorical_accuracy: 0.8924 - val_loss: 0.2891 - val_categorical_accuracy: 0.8984\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 10s 11ms/step - loss: 0.2840 - categorical_accuracy: 0.8975 - val_loss: 0.2765 - val_categorical_accuracy: 0.9023\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 10s 10ms/step - loss: 0.2665 - categorical_accuracy: 0.9025 - val_loss: 0.2628 - val_categorical_accuracy: 0.9104\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 10s 11ms/step - loss: 0.2573 - categorical_accuracy: 0.9069 - val_loss: 0.2664 - val_categorical_accuracy: 0.9018\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 10s 11ms/step - loss: 0.2414 - categorical_accuracy: 0.9113 - val_loss: 0.2605 - val_categorical_accuracy: 0.9106\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 10s 11ms/step - loss: 0.2313 - categorical_accuracy: 0.9156 - val_loss: 0.2808 - val_categorical_accuracy: 0.9102\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 10s 11ms/step - loss: 0.2231 - categorical_accuracy: 0.9186 - val_loss: 0.2585 - val_categorical_accuracy: 0.9108\n",
            "Train elapsed time: 131.75728249549866 seconds\n",
            "INFO:tensorflow:Assets written to: fashion_mnist_model.tf/assets\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,470\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 386\n",
            "_________________________________________________________________\n",
            "None\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.2585 - categorical_accuracy: 0.9108\n",
            "Test elapsed time: 1.4244866371154785 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDdCz-Ir_AdR",
        "outputId": "ec7f1387-abbd-4d6b-be84-37ddf0c98cf8"
      },
      "source": [
        "#model dimension\n",
        "!du -h fashion_mnist_model.tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0K\tfashion_mnist_model.tf/assets\n",
            "19M\tfashion_mnist_model.tf/variables\n",
            "19M\tfashion_mnist_model.tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UctEE8m-siux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dea8751-4baf-45b7-b714-ffc1863c3079"
      },
      "source": [
        "!pip install -q tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██                              | 10kB 27.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 40kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 51kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 61kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 81kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 102kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 112kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 122kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 133kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 143kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 153kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 163kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 7.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz0Y7gqpOqzD"
      },
      "source": [
        "**OPTIMIZATION FUNCTIONS DEFINITION**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzp-KMDfonA2"
      },
      "source": [
        "#1 PRUNING\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "def prune_model(model):\n",
        "  prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "  # Compute end step to finish pruning after 2 epochs.\n",
        "  batch_size = 128\n",
        "  epochs = 2\n",
        "  validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "  num_images = x_train.shape[0] * (1 - validation_split)\n",
        "  end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "  # Define model for pruning.\n",
        "  pruning_params = {\n",
        "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                                final_sparsity=0.80,\n",
        "                                                                begin_step=0,\n",
        "                                                                end_step=end_step)\n",
        "  }\n",
        "\n",
        "  model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "  # `prune_low_magnitude` requires a recompile.\n",
        "  model_for_pruning.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  print(model_for_pruning.summary())\n",
        "\n",
        "  model_for_pruning.save(\"pruned_model.tf\", overwrite=True)\n",
        "  dimension = !du -h pruned_model.tf\n",
        "\n",
        "  print(dimension)\n",
        "\n",
        "  logdir = './'\n",
        "\n",
        "  callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "    tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "  ]\n",
        "\n",
        "  model_for_pruning.fit(x_train, y_train,\n",
        "                    batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
        "                    callbacks=callbacks)\n",
        "  \n",
        "  start_time = time.time()\n",
        "\n",
        "  _, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "    x_test, y_test, verbose=0)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  prune_time = end_time - start_time \n",
        "  print(\"Test elapsed time: {} seconds\".format(end_time - start_time)) \n",
        "  print('Pruned test accuracy:', model_for_pruning_accuracy)\n",
        "\n",
        "  return model, [model_for_pruning_accuracy, prune_time, float(dimension[2].split('M')[0])]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7pXwbfM5VSW"
      },
      "source": [
        "#2 QUANTIZATION\n",
        "\n",
        "def apply_quantization_to_all(layer):\n",
        "  if isinstance(layer, (#tf.keras.layers.Dense\n",
        "                        #tf.keras.layers.Conv2D \n",
        "                        tf.keras.layers.MaxPooling2D, \n",
        "                        tf.keras.layers.Dropout\n",
        "                        )):\n",
        "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "  return layer\n",
        "\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "\n",
        "annotated_model = tf.keras.models.clone_model(\n",
        "    model,\n",
        "    clone_function=apply_quantization_to_all,\n",
        ")\n",
        "\n",
        "def quant_model(model):\n",
        "  quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "  # q_aware stands for for quantization aware.\n",
        "  #q_aware_model = quantize_model(annotated_model)\n",
        "  q_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "\n",
        "  # `quantize_model` requires a recompile.\n",
        "  q_aware_model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  print(q_aware_model.summary())\n",
        "\n",
        "  q_aware_model.save(\"quant_model.tf\", overwrite=True)\n",
        "  dimension = !du -h quant_model.tf \n",
        "  print(dimension)\n",
        "\n",
        "  train_images_subset = x_train[0:1000] # out of 60000\n",
        "  train_labels_subset = y_train[0:1000]\n",
        "\n",
        "  q_aware_model.fit(train_images_subset, train_labels_subset,\n",
        "                    batch_size=500, epochs=1, validation_split=0.1)\n",
        "  \n",
        "  start_time = time.time()\n",
        "\n",
        "  _, q_aware_model_accuracy = q_aware_model.evaluate(\n",
        "    x_test, y_test, verbose=0)\n",
        "\n",
        "  end_time = time.time()\n",
        "  quant_time = end_time - start_time\n",
        "  print(\"Test elapsed time: {} seconds\".format(end_time - start_time)) \n",
        "  print('Pruned test accuracy:', q_aware_model_accuracy)\n",
        "\n",
        "  print(q_aware_model_accuracy, quant_time, dimension[2])\n",
        "\n",
        "  return model, [q_aware_model_accuracy, quant_time, float(dimension[2].split('M')[0])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZJDwdYb-Tet"
      },
      "source": [
        "#3 Weight Clustering\n",
        "\n",
        "def cluster_model(model):\n",
        "  model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "\n",
        "  cluster_weights = tfmot.clustering.keras.cluster_weights\n",
        "  CentroidInitialization = tfmot.clustering.keras.CentroidInitialization\n",
        "\n",
        "  clustering_params = {\n",
        "    'number_of_clusters': 10,\n",
        "    'cluster_centroids_init': CentroidInitialization.LINEAR\n",
        "  }\n",
        "\n",
        "  clustered_model = cluster_weights(model, **clustering_params)\n",
        "\n",
        "  # Use smaller learning rate for fine-tuning clustered model\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "\n",
        "  clustered_model.compile(\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "  print(clustered_model.summary())\n",
        "\n",
        "  # Fine-tune model\n",
        "  clustered_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=500,\n",
        "    epochs=1,\n",
        "    validation_split=0.1)\n",
        "  \n",
        "  start_time = time.time()\n",
        "\n",
        "  _, clustered_model_accuracy = clustered_model.evaluate(\n",
        "    x_test, y_test, verbose=0)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  clust_time = end_time - start_time \n",
        "  print(\"Test elapsed time: {} seconds\".format(end_time - start_time)) \n",
        "  print('Clustered test accuracy:', clustered_model_accuracy)\n",
        "\n",
        "  clustered_model.save(\"cluster_model.tf\", overwrite=True)\n",
        "  dimension = !du -h cluster_model.tf\n",
        "  print(dimension)\n",
        "\n",
        "  return model, [clustered_model_accuracy, clust_time, float(dimension[2].split('M')[0])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2Jw7jfpZyj"
      },
      "source": [
        "#4 Distillation\n",
        "\n",
        "class Distiller(tf.keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=10,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results\n",
        "\n",
        "# Create the student\n",
        "def get_student(input_shape):\n",
        "  student = tf.keras.models.Sequential()\n",
        "\n",
        "  student.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
        "  student.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='elu'))\n",
        "  student.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "  student.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "  student.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
        "  student.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='elu'))\n",
        "  student.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  student.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "  student.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n",
        "  student.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
        "  student.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "  student.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "  student.add(tf.keras.layers.Flatten())\n",
        "  student.add(tf.keras.layers.Dense(256))\n",
        "  student.add(tf.keras.layers.Activation('elu'))\n",
        "  student.add(tf.keras.layers.Dropout(0.5))\n",
        "  student.add(tf.keras.layers.Dense(10))\n",
        "  student.add(tf.keras.layers.Activation('softmax'))\n",
        "\n",
        "  return student\n",
        "\n",
        "def distill_model(model):\n",
        "  teacher = model\n",
        "  student = get_student(x_train.shape[1:])\n",
        "\n",
        "  # Clone student for later comparison\n",
        "  student_scratch = tf.keras.models.clone_model(student)\n",
        "\n",
        "  # Initialize and compile distiller\n",
        "  distiller = Distiller(student=student, teacher=teacher)\n",
        "  distiller.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(),\n",
        "      metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
        "      student_loss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "      distillation_loss_fn=tf.keras.losses.KLDivergence(),\n",
        "      alpha=0.1,\n",
        "      temperature=10,\n",
        "  )\n",
        "\n",
        "  print(x_train.shape, y_train.shape)\n",
        "  # Distill teacher to student\n",
        "  distiller.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "  # Evaluate student on test dataset\n",
        "  distiller.evaluate(x_test, y_test)\n",
        "\n",
        "  return distiller"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x36BMSGbsc8j",
        "outputId": "95f8a4d9-1d1b-4b66-9c5a-0a1b95502f22"
      },
      "source": [
        "#Run Distillation t 10 ep 10\n",
        "## n.b. here I notice poor performance and decide to drop it\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "distilled_model = distill_model(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1) (60000, 10)\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 17s 8ms/step - categorical_accuracy: 0.7887 - student_loss: 0.6583 - distillation_loss: 1.1491e-04\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - categorical_accuracy: 0.8573 - student_loss: 0.4088 - distillation_loss: 6.4139e-05\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - categorical_accuracy: 0.8706 - student_loss: 0.3679 - distillation_loss: 5.4139e-05\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - categorical_accuracy: 0.8812 - student_loss: 0.3381 - distillation_loss: 4.8437e-05\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - categorical_accuracy: 0.8873 - student_loss: 0.3177 - distillation_loss: 4.4678e-05\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - categorical_accuracy: 0.8917 - student_loss: 0.3020 - distillation_loss: 4.2311e-05\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 16s 8ms/step - categorical_accuracy: 0.8978 - student_loss: 0.2842 - distillation_loss: 3.9560e-05\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.9036 - student_loss: 0.2733 - distillation_loss: 3.7488e-05\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.9088 - student_loss: 0.2584 - distillation_loss: 3.5607e-05\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.9138 - student_loss: 0.2471 - distillation_loss: 3.4103e-05\n",
            "313/313 [==============================] - 1s 4ms/step - categorical_accuracy: 0.9102 - student_loss: 0.2839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Voy8CVdyR3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13776327-4396-4722-da1b-ede388cd1a14"
      },
      "source": [
        "#Run Distillation\n",
        "## n.b. here I notice poor performance and decide to drop it\n",
        "distilled_model = distill_model(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1) (60000, 10)\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 17s 9ms/step - categorical_accuracy: 0.7860 - student_loss: 0.6622 - distillation_loss: 1.1683e-04\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.8566 - student_loss: 0.4105 - distillation_loss: 6.3861e-05\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.8719 - student_loss: 0.3595 - distillation_loss: 5.2701e-05\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.8795 - student_loss: 0.3399 - distillation_loss: 4.8572e-05\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.8846 - student_loss: 0.3248 - distillation_loss: 4.6136e-05\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.8940 - student_loss: 0.3026 - distillation_loss: 4.1530e-05\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.9007 - student_loss: 0.2809 - distillation_loss: 3.8032e-05\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.9043 - student_loss: 0.2702 - distillation_loss: 3.7162e-05\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.9096 - student_loss: 0.2569 - distillation_loss: 3.5527e-05\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - categorical_accuracy: 0.9156 - student_loss: 0.2407 - distillation_loss: 3.3781e-05\n",
            "313/313 [==============================] - 1s 4ms/step - categorical_accuracy: 0.9040 - student_loss: 0.3051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSwnU1CjPYsH"
      },
      "source": [
        "**PHASE 1 - BASELINE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKc86pv1Ngua",
        "outputId": "3402d844-2a50-48d9-a0cf-37ed6c764f43"
      },
      "source": [
        "# BASELINE\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "start_time = time.time()\n",
        "\n",
        "_, model_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "baseline_time = end_time - start_time \n",
        "print('Baseline test accuracy:', model_accuracy)\n",
        "print(\"Test elapsed time: {} seconds\".format(end_time - start_time))\n",
        "\n",
        "baseline_dim = !du -h fashion_mnist_model.tf\n",
        "baseline_dim = float(baseline_dim[2].split('M')[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.2585 - categorical_accuracy: 0.9108\n",
            "Baseline test accuracy: 0.9107999801635742\n",
            "Test elapsed time: 1.434041976928711 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbvjqRC4mV1d"
      },
      "source": [
        "**PHASE 2 - REDUCE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3xCVCHPNjQq",
        "outputId": "7dafed41-b6ba-4ac9-8bdb-7cc9f6bc6af7"
      },
      "source": [
        "# REDUX (model with reduced dimensions)\n",
        "train(x_train, y_train, x_test, y_test, redux = True)\n",
        "#test(x_test, y_test, redux = True)\n",
        "\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model_redux.tf\")\n",
        "start_time = time.time()\n",
        "\n",
        "_, redux_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "redux_time = end_time - start_time \n",
        "print('Redux test accuracy:', model_accuracy)\n",
        "print(\"Test elapsed time: {} seconds\".format(end_time - start_time))\n",
        "\n",
        "redux_dim = !du -h fashion_mnist_model_redux.tf\n",
        "redux_dim = float(redux_dim[2].split('M')[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_9 (Batch (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 14, 14, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               147584    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 406,286\n",
            "Trainable params: 406,092\n",
            "Non-trainable params: 194\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "938/938 [==============================] - 7s 6ms/step - loss: 0.6281 - categorical_accuracy: 0.7887 - val_loss: 0.3956 - val_categorical_accuracy: 0.8538\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.3775 - categorical_accuracy: 0.8634 - val_loss: 0.3078 - val_categorical_accuracy: 0.8856\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.3282 - categorical_accuracy: 0.8815 - val_loss: 0.2845 - val_categorical_accuracy: 0.8963\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.3010 - categorical_accuracy: 0.8900 - val_loss: 0.2781 - val_categorical_accuracy: 0.8981\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.2842 - categorical_accuracy: 0.8968 - val_loss: 0.2953 - val_categorical_accuracy: 0.8954\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.2689 - categorical_accuracy: 0.9020 - val_loss: 0.2556 - val_categorical_accuracy: 0.9077\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.2567 - categorical_accuracy: 0.9060 - val_loss: 0.2756 - val_categorical_accuracy: 0.8998\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.2453 - categorical_accuracy: 0.9099 - val_loss: 0.2579 - val_categorical_accuracy: 0.9082\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.2355 - categorical_accuracy: 0.9132 - val_loss: 0.2557 - val_categorical_accuracy: 0.9126\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.2288 - categorical_accuracy: 0.9169 - val_loss: 0.2456 - val_categorical_accuracy: 0.9131\n",
            "Train elapsed time: 58.95391821861267 seconds\n",
            "INFO:tensorflow:Assets written to: fashion_mnist_model_redux.tf/assets\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2456 - categorical_accuracy: 0.9131\n",
            "Redux test accuracy: 0.9107999801635742\n",
            "Test elapsed time: 1.163611650466919 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQjs3BopmPHr"
      },
      "source": [
        "**PHASE 3 - OPTIMIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXEUICcqtIA7",
        "outputId": "533722e6-0c40-42f5-de48-7e9f05518a82"
      },
      "source": [
        "#PRUNING\n",
        "\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "\n",
        "pruned_model, pruned_results = prune_model(model)\n",
        "\n",
        "prune_dim = !du -h pruned_model.tf\n",
        "prune_dim = float(prune_dim[2].split('M')[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2191: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_batch_no (None, 28, 28, 1)         5         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d ( (None, 28, 28, 64)        3266      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout  (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 14, 14, 64)        257       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 14, 14, 128)       409730    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 7, 7, 128)         513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 7, 7, 256)         1638658   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten  (None, 2304)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense (P (None, 256)               1179906   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_1  (None, 10)                5132      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 10)                1         \n",
            "=================================================================\n",
            "Total params: 3,237,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 1,618,393\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tpruned_model.tf/assets', '13M\\tpruned_model.tf/variables', '15M\\tpruned_model.tf']\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  5/422 [..............................] - ETA: 1:13 - loss: 0.1915 - accuracy: 0.9266WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_begin` time: 0.0526s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_begin` time: 0.0526s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_end` time: 0.0579s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_end` time: 0.0579s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "422/422 [==============================] - 16s 31ms/step - loss: 0.2162 - accuracy: 0.9198 - val_loss: 0.1648 - val_accuracy: 0.9395\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2241 - accuracy: 0.9200 - val_loss: 0.1593 - val_accuracy: 0.9385\n",
            "Test elapsed time: 0.9480061531066895 seconds\n",
            "Pruned test accuracy: 0.9146000146865845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5aqxGelugV7",
        "outputId": "2dbb04ad-d15e-4eda-b9ff-74f521b53447"
      },
      "source": [
        "#QUANTIZATION\n",
        "\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "quantized_model, quantized_results = quant_model(model)\n",
        "\n",
        "quant_dim = !du -h quant_model.tf\n",
        "quant_dim = float(quant_dim[2].split('M')[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_1 (QuantizeWra (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_2 (QuantizeWra (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "quant_dropout_3 (QuantizeWra (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn, dropout_1_layer_call_and_return_conditional_losses, dropout_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tquant_model.tf/assets', '6.2M\\tquant_model.tf/variables', '6.7M\\tquant_model.tf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 3s 1s/step - loss: 0.1603 - accuracy: 0.9422 - val_loss: 0.1530 - val_accuracy: 0.9300\n",
            "Test elapsed time: 0.9212069511413574 seconds\n",
            "Pruned test accuracy: 0.910099983215332\n",
            "0.910099983215332 0.9212069511413574 6.7M\tquant_model.tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MDVlFdQvTtc",
        "outputId": "e1ccd136-9c01-4cac-da5a-6a818f52284d"
      },
      "source": [
        "#CLUSTER\n",
        "\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "clusterd_model, clustered_results = cluster_model(model)\n",
        "\n",
        "clust_dim = !du -h cluster_model.tf\n",
        "clust_dim = float(clust_dim[2].split('M')[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cluster_batch_normalization  (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "cluster_conv2d (ClusterWeigh (None, 28, 28, 64)        1674      \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d (Clust (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout (ClusterWeig (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_1 (ClusterWei (None, 14, 14, 128)       204938    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_1 (Clu (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_1 (ClusterWe (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_2 (ClusterWei (None, 7, 7, 256)         819466    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_2 (Clu (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_2 (ClusterWe (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_flatten (ClusterWeig (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "cluster_dense (ClusterWeight (None, 256)               590090    \n",
            "_________________________________________________________________\n",
            "cluster_activation (ClusterW (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_3 (ClusterWe (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dense_1 (ClusterWeig (None, 10)                2580      \n",
            "_________________________________________________________________\n",
            "cluster_activation_1 (Cluste (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,520\n",
            "Trainable params: 1,150\n",
            "Non-trainable params: 1,618,370\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "108/108 [==============================] - 8s 64ms/step - loss: 0.1568 - accuracy: 0.9378 - val_loss: 0.1498 - val_accuracy: 0.9378\n",
            "Test elapsed time: 1.6574249267578125 seconds\n",
            "Clustered test accuracy: 0.9059000015258789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tcluster_model.tf/assets', '13M\\tcluster_model.tf/variables', '13M\\tcluster_model.tf']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjbGHw5MPkcP"
      },
      "source": [
        "**COMBINATION OF OPTIMIZATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OJTvWFmOI2t",
        "outputId": "74c5986d-7620-4c7e-f773-ae1cc225a767"
      },
      "source": [
        "#OPTIMIZATIONS ON BASELINE\n",
        "\n",
        "#QUANT + PRUNE\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "qp_model, qp_results = quant_model(model)\n",
        "qp_model, qp_results = prune_model(qp_model)\n",
        "\n",
        "#PRUNE + QUANT\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "pq_model, pq_results = prune_model(model)\n",
        "pq_model, pq_results = quant_model(pq_model)\n",
        "\n",
        "#QUANT + CLUST\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "qc_model, qc_results = quant_model(model)\n",
        "qc_model, qc_results = cluster_model(qc_model)\n",
        "\n",
        "#CLUST + QUANT\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "cq_model, cq_results = cluster_model(model)\n",
        "cq_model, cq_results = quant_model(cq_model)\n",
        "\n",
        "#PRUNE + CLUST\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "pc_model, pc_results = prune_model(model)\n",
        "pc_model, pc_results = cluster_model(pc_model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_1 (QuantizeWra (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_2 (QuantizeWra (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "quant_dropout_3 (QuantizeWra (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn, dropout_1_layer_call_and_return_conditional_losses, dropout_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tquant_model.tf/assets', '6.2M\\tquant_model.tf/variables', '6.7M\\tquant_model.tf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 270ms/step - loss: 0.1567 - accuracy: 0.9411 - val_loss: 0.1661 - val_accuracy: 0.9200\n",
            "Test elapsed time: 1.304358959197998 seconds\n",
            "Pruned test accuracy: 0.9168000221252441\n",
            "0.9168000221252441 1.304358959197998 6.7M\tquant_model.tf\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_batch_no (None, 28, 28, 1)         5         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d ( (None, 28, 28, 64)        3266      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout  (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 14, 14, 64)        257       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 14, 14, 128)       409730    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 7, 7, 128)         513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 7, 7, 256)         1638658   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten  (None, 2304)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense (P (None, 256)               1179906   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_1  (None, 10)                5132      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 10)                1         \n",
            "=================================================================\n",
            "Total params: 3,237,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 1,618,393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2191: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tpruned_model.tf/assets', '13M\\tpruned_model.tf/variables', '15M\\tpruned_model.tf']\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  5/422 [..............................] - ETA: 1:55 - loss: 0.1911 - accuracy: 0.9219WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0185s vs `on_train_batch_begin` time: 0.0510s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0185s vs `on_train_batch_begin` time: 0.0510s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0185s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0185s vs `on_train_batch_end` time: 0.1260s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "422/422 [==============================] - 16s 31ms/step - loss: 0.2214 - accuracy: 0.9189 - val_loss: 0.1785 - val_accuracy: 0.9315\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2250 - accuracy: 0.9195 - val_loss: 0.1712 - val_accuracy: 0.9338\n",
            "Test elapsed time: 0.9943017959594727 seconds\n",
            "Pruned test accuracy: 0.9121999740600586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2191: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_batch_no (None, 28, 28, 1)         5         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d ( (None, 28, 28, 64)        3266      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout  (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 14, 14, 64)        257       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 14, 14, 128)       409730    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 7, 7, 128)         513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 7, 7, 256)         1638658   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten  (None, 2304)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense (P (None, 256)               1179906   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_1  (None, 10)                5132      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 10)                1         \n",
            "=================================================================\n",
            "Total params: 3,237,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 1,618,393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tpruned_model.tf/assets', '13M\\tpruned_model.tf/variables', '15M\\tpruned_model.tf']\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  5/422 [..............................] - ETA: 1:34 - loss: 0.1882 - accuracy: 0.9250WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0167s vs `on_train_batch_begin` time: 0.0492s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0167s vs `on_train_batch_begin` time: 0.0492s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0167s vs `on_train_batch_end` time: 0.0949s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0167s vs `on_train_batch_end` time: 0.0949s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "422/422 [==============================] - 16s 30ms/step - loss: 0.2235 - accuracy: 0.9188 - val_loss: 0.1743 - val_accuracy: 0.9343\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2259 - accuracy: 0.9173 - val_loss: 0.1603 - val_accuracy: 0.9362\n",
            "Test elapsed time: 0.9123210906982422 seconds\n",
            "Pruned test accuracy: 0.9187999963760376\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_1 (QuantizeWra (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_2 (QuantizeWra (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "quant_dropout_3 (QuantizeWra (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn, dropout_1_layer_call_and_return_conditional_losses, dropout_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tquant_model.tf/assets', '6.2M\\tquant_model.tf/variables', '6.7M\\tquant_model.tf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 272ms/step - loss: 0.1591 - accuracy: 0.9478 - val_loss: 0.1592 - val_accuracy: 0.9300\n",
            "Test elapsed time: 1.3078632354736328 seconds\n",
            "Pruned test accuracy: 0.9171000123023987\n",
            "0.9171000123023987 1.3078632354736328 6.7M\tquant_model.tf\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_1 (QuantizeWra (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_2 (QuantizeWra (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "quant_dropout_3 (QuantizeWra (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn, dropout_1_layer_call_and_return_conditional_losses, dropout_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tquant_model.tf/assets', '6.2M\\tquant_model.tf/variables', '6.7M\\tquant_model.tf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 269ms/step - loss: 0.1680 - accuracy: 0.9344 - val_loss: 0.1511 - val_accuracy: 0.9200\n",
            "Test elapsed time: 1.3012745380401611 seconds\n",
            "Pruned test accuracy: 0.9160000085830688\n",
            "0.9160000085830688 1.3012745380401611 6.7M\tquant_model.tf\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cluster_batch_normalization  (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "cluster_conv2d (ClusterWeigh (None, 28, 28, 64)        1674      \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d (Clust (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout (ClusterWeig (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_1 (ClusterWei (None, 14, 14, 128)       204938    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_1 (Clu (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_1 (ClusterWe (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_2 (ClusterWei (None, 7, 7, 256)         819466    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_2 (Clu (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_2 (ClusterWe (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_flatten (ClusterWeig (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "cluster_dense (ClusterWeight (None, 256)               590090    \n",
            "_________________________________________________________________\n",
            "cluster_activation (ClusterW (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_3 (ClusterWe (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dense_1 (ClusterWeig (None, 10)                2580      \n",
            "_________________________________________________________________\n",
            "cluster_activation_1 (Cluste (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,520\n",
            "Trainable params: 1,150\n",
            "Non-trainable params: 1,618,370\n",
            "_________________________________________________________________\n",
            "None\n",
            "108/108 [==============================] - 8s 63ms/step - loss: 0.1568 - accuracy: 0.9382 - val_loss: 0.1498 - val_accuracy: 0.9380\n",
            "Test elapsed time: 2.743138551712036 seconds\n",
            "Clustered test accuracy: 0.906000018119812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tcluster_model.tf/assets', '13M\\tcluster_model.tf/variables', '13M\\tcluster_model.tf']\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cluster_batch_normalization  (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "cluster_conv2d (ClusterWeigh (None, 28, 28, 64)        1674      \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d (Clust (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout (ClusterWeig (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_1 (ClusterWei (None, 14, 14, 128)       204938    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_1 (Clu (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_1 (ClusterWe (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_2 (ClusterWei (None, 7, 7, 256)         819466    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_2 (Clu (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_2 (ClusterWe (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_flatten (ClusterWeig (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "cluster_dense (ClusterWeight (None, 256)               590090    \n",
            "_________________________________________________________________\n",
            "cluster_activation (ClusterW (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_3 (ClusterWe (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dense_1 (ClusterWeig (None, 10)                2580      \n",
            "_________________________________________________________________\n",
            "cluster_activation_1 (Cluste (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,520\n",
            "Trainable params: 1,150\n",
            "Non-trainable params: 1,618,370\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "108/108 [==============================] - 8s 63ms/step - loss: 0.1567 - accuracy: 0.9381 - val_loss: 0.1496 - val_accuracy: 0.9382\n",
            "Test elapsed time: 3.1891367435455322 seconds\n",
            "Clustered test accuracy: 0.9059000015258789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tcluster_model.tf/assets', '13M\\tcluster_model.tf/variables', '13M\\tcluster_model.tf']\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_1 (QuantizeWra (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_2 (QuantizeWra (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "quant_dropout_3 (QuantizeWra (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn, dropout_1_layer_call_and_return_conditional_losses, dropout_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tquant_model.tf/assets', '6.2M\\tquant_model.tf/variables', '6.7M\\tquant_model.tf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 264ms/step - loss: 0.1616 - accuracy: 0.9378 - val_loss: 0.1629 - val_accuracy: 0.9500\n",
            "Test elapsed time: 0.9545917510986328 seconds\n",
            "Pruned test accuracy: 0.9182999730110168\n",
            "0.9182999730110168 0.9545917510986328 6.7M\tquant_model.tf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2191: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_batch_no (None, 28, 28, 1)         5         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d ( (None, 28, 28, 64)        3266      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout  (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 14, 14, 64)        257       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 14, 14, 128)       409730    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 7, 7, 128)         513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 7, 7, 256)         1638658   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten  (None, 2304)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense (P (None, 256)               1179906   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_1  (None, 10)                5132      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 10)                1         \n",
            "=================================================================\n",
            "Total params: 3,237,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 1,618,393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tpruned_model.tf/assets', '13M\\tpruned_model.tf/variables', '15M\\tpruned_model.tf']\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  5/422 [..............................] - ETA: 2:12 - loss: 0.2264 - accuracy: 0.9094WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0167s vs `on_train_batch_begin` time: 0.0506s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0167s vs `on_train_batch_begin` time: 0.0506s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0167s vs `on_train_batch_end` time: 0.1539s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0167s vs `on_train_batch_end` time: 0.1539s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "422/422 [==============================] - 16s 31ms/step - loss: 0.2163 - accuracy: 0.9202 - val_loss: 0.1741 - val_accuracy: 0.9293\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2222 - accuracy: 0.9196 - val_loss: 0.1535 - val_accuracy: 0.9410\n",
            "Test elapsed time: 0.9208343029022217 seconds\n",
            "Pruned test accuracy: 0.9193999767303467\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cluster_batch_normalization  (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "cluster_conv2d (ClusterWeigh (None, 28, 28, 64)        1674      \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d (Clust (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout (ClusterWeig (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_1 (ClusterWei (None, 14, 14, 128)       204938    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_1 (Clu (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_1 (ClusterWe (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_2 (ClusterWei (None, 7, 7, 256)         819466    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_2 (Clu (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_2 (ClusterWe (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_flatten (ClusterWeig (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "cluster_dense (ClusterWeight (None, 256)               590090    \n",
            "_________________________________________________________________\n",
            "cluster_activation (ClusterW (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_3 (ClusterWe (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dense_1 (ClusterWeig (None, 10)                2580      \n",
            "_________________________________________________________________\n",
            "cluster_activation_1 (Cluste (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,520\n",
            "Trainable params: 1,150\n",
            "Non-trainable params: 1,618,370\n",
            "_________________________________________________________________\n",
            "None\n",
            "108/108 [==============================] - 8s 63ms/step - loss: 0.1567 - accuracy: 0.9381 - val_loss: 0.1498 - val_accuracy: 0.9377\n",
            "Test elapsed time: 2.7426393032073975 seconds\n",
            "Clustered test accuracy: 0.906000018119812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tcluster_model.tf/assets', '13M\\tcluster_model.tf/variables', '13M\\tcluster_model.tf']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaKVEtyJyq2K",
        "outputId": "6c22bacd-f27b-44aa-a541-1059be8603b9"
      },
      "source": [
        "## OPTIMIZATIONS ON REDUX\n",
        "\n",
        "#QUANT + PRUNE\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model_redux.tf\")\n",
        "rqp_model, rqp_results = quant_model(model)\n",
        "rqp_model, rqp_results = prune_model(rqp_model)\n",
        "\n",
        "#PRUNE + QUANT\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model_redux.tf\")\n",
        "rpq_model, rpq_results = prune_model(model)\n",
        "rpq_model, rpq_results = quant_model(rpq_model)\n",
        "\n",
        "#QUANT + CLUST\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model_redux.tf\")\n",
        "rqc_model, rqc_results = quant_model(model)\n",
        "rqc_model, rqc_results = cluster_model(rqc_model)\n",
        "\n",
        "#CLUST + QUANT\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model_redux.tf\")\n",
        "rcq_model, rcq_results = cluster_model(model)\n",
        "rcq_model, rcq_results = quant_model(rcq_model)\n",
        "\n",
        "#PRUNE + CLUST\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model_redux.tf\")\n",
        "rpc_model, rpc_results = prune_model(model)\n",
        "rpc_model, rpc_results = cluster_model(rpc_model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_1 (QuantizeWra (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_2 (QuantizeWra (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "quant_dropout_3 (QuantizeWra (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn, dropout_1_layer_call_and_return_conditional_losses, dropout_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tquant_model.tf/assets', '6.2M\\tquant_model.tf/variables', '6.7M\\tquant_model.tf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 270ms/step - loss: 0.1787 - accuracy: 0.9322 - val_loss: 0.1501 - val_accuracy: 0.9200\n",
            "Test elapsed time: 1.2998781204223633 seconds\n",
            "Pruned test accuracy: 0.9154000282287598\n",
            "0.9154000282287598 1.2998781204223633 6.7M\tquant_model.tf\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_batch_no (None, 28, 28, 1)         5         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_9 (None, 28, 28, 32)        1634      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 14, 14, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 14, 14, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 14, 14, 32)        129       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 14, 14, 64)        102466    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 7, 7, 64)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 7, 7, 64)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 7, 7, 64)          257       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 7, 7, 128)         409730    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 3, 3, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 3, 3, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten_ (None, 1152)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_6  (None, 128)               295042    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 128)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 128)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_7  (None, 10)                2572      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 10)                1         \n",
            "=================================================================\n",
            "Total params: 811,845\n",
            "Trainable params: 406,092\n",
            "Non-trainable params: 405,753\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2191: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "WARNING:absl:Found untraced functions such as conv2d_9_layer_call_fn, conv2d_9_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, conv2d_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tpruned_model.tf/assets', '3.2M\\tpruned_model.tf/variables', '5.0M\\tpruned_model.tf']\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "422/422 [==============================] - 13s 23ms/step - loss: 0.2663 - accuracy: 0.9030 - val_loss: 0.2099 - val_accuracy: 0.9215\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 8s 19ms/step - loss: 0.2879 - accuracy: 0.8991 - val_loss: 0.1909 - val_accuracy: 0.9290\n",
            "Test elapsed time: 0.6957435607910156 seconds\n",
            "Pruned test accuracy: 0.9099000096321106\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2191: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_batch_no (None, 28, 28, 1)         5         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_9 (None, 28, 28, 32)        1634      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 14, 14, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 14, 14, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 14, 14, 32)        129       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 14, 14, 64)        102466    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 7, 7, 64)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 7, 7, 64)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 7, 7, 64)          257       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 7, 7, 128)         409730    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 3, 3, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 3, 3, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten_ (None, 1152)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_6  (None, 128)               295042    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 128)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 128)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_7  (None, 10)                2572      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 10)                1         \n",
            "=================================================================\n",
            "Total params: 811,845\n",
            "Trainable params: 406,092\n",
            "Non-trainable params: 405,753\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_9_layer_call_fn, conv2d_9_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, conv2d_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tpruned_model.tf/assets', '3.2M\\tpruned_model.tf/variables', '5.0M\\tpruned_model.tf']\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "422/422 [==============================] - 14s 23ms/step - loss: 0.2624 - accuracy: 0.9034 - val_loss: 0.2091 - val_accuracy: 0.9227\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 8s 19ms/step - loss: 0.2803 - accuracy: 0.9010 - val_loss: 0.1905 - val_accuracy: 0.9320\n",
            "Test elapsed time: 1.3004262447357178 seconds\n",
            "Pruned test accuracy: 0.9100000262260437\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_1 (QuantizeWra (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_2 (QuantizeWra (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "quant_dropout_3 (QuantizeWra (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn, dropout_1_layer_call_and_return_conditional_losses, dropout_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tquant_model.tf/assets', '6.2M\\tquant_model.tf/variables', '6.7M\\tquant_model.tf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 263ms/step - loss: 0.1610 - accuracy: 0.9400 - val_loss: 0.1621 - val_accuracy: 0.9300\n",
            "Test elapsed time: 1.3012731075286865 seconds\n",
            "Pruned test accuracy: 0.9139000177383423\n",
            "0.9139000177383423 1.3012731075286865 6.7M\tquant_model.tf\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_1 (QuantizeWra (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_2 (QuantizeWra (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "quant_dropout_3 (QuantizeWra (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn, dropout_1_layer_call_and_return_conditional_losses, dropout_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tquant_model.tf/assets', '6.2M\\tquant_model.tf/variables', '6.7M\\tquant_model.tf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 259ms/step - loss: 0.1703 - accuracy: 0.9411 - val_loss: 0.1607 - val_accuracy: 0.9000\n",
            "Test elapsed time: 1.3007524013519287 seconds\n",
            "Pruned test accuracy: 0.9096999764442444\n",
            "0.9096999764442444 1.3007524013519287 6.7M\tquant_model.tf\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cluster_batch_normalization  (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "cluster_conv2d (ClusterWeigh (None, 28, 28, 64)        1674      \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d (Clust (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout (ClusterWeig (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_1 (ClusterWei (None, 14, 14, 128)       204938    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_1 (Clu (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_1 (ClusterWe (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_2 (ClusterWei (None, 7, 7, 256)         819466    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_2 (Clu (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_2 (ClusterWe (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_flatten (ClusterWeig (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "cluster_dense (ClusterWeight (None, 256)               590090    \n",
            "_________________________________________________________________\n",
            "cluster_activation (ClusterW (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_3 (ClusterWe (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dense_1 (ClusterWeig (None, 10)                2580      \n",
            "_________________________________________________________________\n",
            "cluster_activation_1 (Cluste (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,520\n",
            "Trainable params: 1,150\n",
            "Non-trainable params: 1,618,370\n",
            "_________________________________________________________________\n",
            "None\n",
            "108/108 [==============================] - 8s 61ms/step - loss: 0.1568 - accuracy: 0.9381 - val_loss: 0.1498 - val_accuracy: 0.9380\n",
            "Test elapsed time: 2.7485101222991943 seconds\n",
            "Clustered test accuracy: 0.906000018119812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tcluster_model.tf/assets', '13M\\tcluster_model.tf/variables', '13M\\tcluster_model.tf']\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cluster_batch_normalization  (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "cluster_conv2d (ClusterWeigh (None, 28, 28, 64)        1674      \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d (Clust (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout (ClusterWeig (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_1 (ClusterWei (None, 14, 14, 128)       204938    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_1 (Clu (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_1 (ClusterWe (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_2 (ClusterWei (None, 7, 7, 256)         819466    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_2 (Clu (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_2 (ClusterWe (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_flatten (ClusterWeig (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "cluster_dense (ClusterWeight (None, 256)               590090    \n",
            "_________________________________________________________________\n",
            "cluster_activation (ClusterW (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_3 (ClusterWe (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dense_1 (ClusterWeig (None, 10)                2580      \n",
            "_________________________________________________________________\n",
            "cluster_activation_1 (Cluste (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,520\n",
            "Trainable params: 1,150\n",
            "Non-trainable params: 1,618,370\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "108/108 [==============================] - 7s 60ms/step - loss: 0.1566 - accuracy: 0.9377 - val_loss: 0.1498 - val_accuracy: 0.9382\n",
            "Test elapsed time: 2.7570245265960693 seconds\n",
            "Clustered test accuracy: 0.9060999751091003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tcluster_model.tf/assets', '13M\\tcluster_model.tf/variables', '13M\\tcluster_model.tf']\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_1 (QuantizeWra (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_2 (QuantizeWra (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "quant_dropout_3 (QuantizeWra (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn, dropout_1_layer_call_and_return_conditional_losses, dropout_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tquant_model.tf/assets', '6.2M\\tquant_model.tf/variables', '6.7M\\tquant_model.tf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 259ms/step - loss: 0.1774 - accuracy: 0.9344 - val_loss: 0.1509 - val_accuracy: 0.9300\n",
            "Test elapsed time: 1.3011274337768555 seconds\n",
            "Pruned test accuracy: 0.9143000245094299\n",
            "0.9143000245094299 1.3011274337768555 6.7M\tquant_model.tf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2191: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_batch_no (None, 28, 28, 1)         5         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_9 (None, 28, 28, 32)        1634      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 14, 14, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 14, 14, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 14, 14, 32)        129       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 14, 14, 64)        102466    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 7, 7, 64)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 7, 7, 64)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 7, 7, 64)          257       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 7, 7, 128)         409730    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 3, 3, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 3, 3, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten_ (None, 1152)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_6  (None, 128)               295042    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 128)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 128)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_7  (None, 10)                2572      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 10)                1         \n",
            "=================================================================\n",
            "Total params: 811,845\n",
            "Trainable params: 406,092\n",
            "Non-trainable params: 405,753\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_9_layer_call_fn, conv2d_9_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, conv2d_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tpruned_model.tf/assets', '3.2M\\tpruned_model.tf/variables', '5.0M\\tpruned_model.tf']\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "422/422 [==============================] - 14s 24ms/step - loss: 0.2647 - accuracy: 0.9039 - val_loss: 0.2066 - val_accuracy: 0.9222\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 8s 20ms/step - loss: 0.2782 - accuracy: 0.9004 - val_loss: 0.1896 - val_accuracy: 0.9298\n",
            "Test elapsed time: 1.2994587421417236 seconds\n",
            "Pruned test accuracy: 0.9103999733924866\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cluster_batch_normalization  (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "cluster_conv2d (ClusterWeigh (None, 28, 28, 64)        1674      \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d (Clust (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout (ClusterWeig (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_1 (ClusterWei (None, 14, 14, 128)       204938    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_1 (Clu (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_1 (ClusterWe (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_2 (ClusterWei (None, 7, 7, 256)         819466    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_2 (Clu (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_2 (ClusterWe (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_flatten (ClusterWeig (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "cluster_dense (ClusterWeight (None, 256)               590090    \n",
            "_________________________________________________________________\n",
            "cluster_activation (ClusterW (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_3 (ClusterWe (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dense_1 (ClusterWeig (None, 10)                2580      \n",
            "_________________________________________________________________\n",
            "cluster_activation_1 (Cluste (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,520\n",
            "Trainable params: 1,150\n",
            "Non-trainable params: 1,618,370\n",
            "_________________________________________________________________\n",
            "None\n",
            "108/108 [==============================] - 8s 62ms/step - loss: 0.1566 - accuracy: 0.9378 - val_loss: 0.1496 - val_accuracy: 0.9382\n",
            "Test elapsed time: 1.7787938117980957 seconds\n",
            "Clustered test accuracy: 0.906000018119812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tcluster_model.tf/assets', '13M\\tcluster_model.tf/variables', '13M\\tcluster_model.tf']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apl8DDna4ag8",
        "outputId": "24117a0a-54ee-483d-ca07-3cc6e9f6ee77"
      },
      "source": [
        "# All OPT\n",
        "\n",
        "# q + p + c\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\")\n",
        "qpc_model, qpc_results = quant_model(model)\n",
        "qpc_model, qpc_results = prune_model(qpc_model)\n",
        "qpc_model, qpc_results = cluster_model(qpc_model)\n",
        "\n",
        "# r + q + p + c\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model_redux.tf\")\n",
        "rqpc_model, rqpc_results = quant_model(model)\n",
        "rqpc_model, rqpc_results = prune_model(rqpc_model)\n",
        "rqpc_model, rqpc_results = cluster_model(rqpc_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_1 (QuantizeWra (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_2 (QuantizeWra (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "quant_dropout_3 (QuantizeWra (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn, dropout_1_layer_call_and_return_conditional_losses, dropout_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tquant_model.tf/assets', '6.2M\\tquant_model.tf/variables', '6.7M\\tquant_model.tf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 259ms/step - loss: 0.1656 - accuracy: 0.9389 - val_loss: 0.1657 - val_accuracy: 0.9500\n",
            "Test elapsed time: 1.3017518520355225 seconds\n",
            "Pruned test accuracy: 0.9172999858856201\n",
            "0.9172999858856201 1.3017518520355225 6.7M\tquant_model.tf\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_batch_no (None, 28, 28, 1)         5         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d ( (None, 28, 28, 64)        3266      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout  (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 14, 14, 64)        257       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 14, 14, 128)       409730    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 7, 7, 128)         513       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 7, 7, 256)         1638658   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten  (None, 2304)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense (P (None, 256)               1179906   \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_1  (None, 10)                5132      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 10)                1         \n",
            "=================================================================\n",
            "Total params: 3,237,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 1,618,393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2191: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tpruned_model.tf/assets', '13M\\tpruned_model.tf/variables', '15M\\tpruned_model.tf']\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  5/422 [..............................] - ETA: 1:51 - loss: 0.2350 - accuracy: 0.9141WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0162s vs `on_train_batch_begin` time: 0.0512s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0162s vs `on_train_batch_begin` time: 0.0512s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0162s vs `on_train_batch_end` time: 0.1208s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0162s vs `on_train_batch_end` time: 0.1208s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "422/422 [==============================] - 16s 30ms/step - loss: 0.2176 - accuracy: 0.9206 - val_loss: 0.1766 - val_accuracy: 0.9330\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2215 - accuracy: 0.9200 - val_loss: 0.1540 - val_accuracy: 0.9410\n",
            "Test elapsed time: 1.0054974555969238 seconds\n",
            "Pruned test accuracy: 0.917900025844574\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cluster_batch_normalization  (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "cluster_conv2d (ClusterWeigh (None, 28, 28, 64)        1674      \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d (Clust (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout (ClusterWeig (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_1 (ClusterWei (None, 14, 14, 128)       204938    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_1 (Clu (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_1 (ClusterWe (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_2 (ClusterWei (None, 7, 7, 256)         819466    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_2 (Clu (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_2 (ClusterWe (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_flatten (ClusterWeig (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "cluster_dense (ClusterWeight (None, 256)               590090    \n",
            "_________________________________________________________________\n",
            "cluster_activation (ClusterW (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_3 (ClusterWe (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dense_1 (ClusterWeig (None, 10)                2580      \n",
            "_________________________________________________________________\n",
            "cluster_activation_1 (Cluste (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,520\n",
            "Trainable params: 1,150\n",
            "Non-trainable params: 1,618,370\n",
            "_________________________________________________________________\n",
            "None\n",
            "108/108 [==============================] - 7s 61ms/step - loss: 0.1569 - accuracy: 0.9377 - val_loss: 0.1499 - val_accuracy: 0.9377\n",
            "Test elapsed time: 2.751671075820923 seconds\n",
            "Clustered test accuracy: 0.9057000279426575\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tcluster_model.tf/assets', '13M\\tcluster_model.tf/variables', '13M\\tcluster_model.tf']\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        1664      \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d (Quantiz (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "quant_dropout (QuantizeWrapp (None, 14, 14, 64)        1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       204928    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_1 (Quant (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_1 (QuantizeWra (None, 7, 7, 128)         1         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 256)         819456    \n",
            "_________________________________________________________________\n",
            "quant_max_pooling2d_2 (Quant (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "quant_dropout_2 (QuantizeWra (None, 3, 3, 256)         1         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               590080    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "quant_dropout_3 (QuantizeWra (None, 256)               1         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,477\n",
            "Trainable params: 1,619,084\n",
            "Non-trainable params: 393\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn, dropout_1_layer_call_and_return_conditional_losses, dropout_2_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: quant_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tquant_model.tf/assets', '6.2M\\tquant_model.tf/variables', '6.7M\\tquant_model.tf']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 275ms/step - loss: 0.1698 - accuracy: 0.9333 - val_loss: 0.1691 - val_accuracy: 0.9500\n",
            "Test elapsed time: 1.298142671585083 seconds\n",
            "Pruned test accuracy: 0.9172999858856201\n",
            "0.9172999858856201 1.298142671585083 6.7M\tquant_model.tf\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_batch_no (None, 28, 28, 1)         5         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_9 (None, 28, 28, 32)        1634      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 14, 14, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 14, 14, 32)        1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 14, 14, 32)        129       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 14, 14, 64)        102466    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 7, 7, 64)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 7, 7, 64)          1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_batch_no (None, 7, 7, 64)          257       \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 7, 7, 128)         409730    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_max_pool (None, 3, 3, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 3, 3, 128)         1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_flatten_ (None, 1152)              1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_6  (None, 128)               295042    \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 128)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dropout_ (None, 128)               1         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_7  (None, 10)                2572      \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_activati (None, 10)                1         \n",
            "=================================================================\n",
            "Total params: 811,845\n",
            "Trainable params: 406,092\n",
            "Non-trainable params: 405,753\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2191: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "WARNING:absl:Found untraced functions such as conv2d_9_layer_call_fn, conv2d_9_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, conv2d_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: pruned_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tpruned_model.tf/assets', '3.2M\\tpruned_model.tf/variables', '5.0M\\tpruned_model.tf']\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  5/422 [..............................] - ETA: 1:47 - loss: 0.2517 - accuracy: 0.8969WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_begin` time: 0.0528s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_begin` time: 0.0528s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_end` time: 0.1148s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_end` time: 0.1148s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "422/422 [==============================] - 14s 23ms/step - loss: 0.2593 - accuracy: 0.9053 - val_loss: 0.2091 - val_accuracy: 0.9200\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 8s 19ms/step - loss: 0.2805 - accuracy: 0.8999 - val_loss: 0.1924 - val_accuracy: 0.9290\n",
            "Test elapsed time: 1.3016774654388428 seconds\n",
            "Pruned test accuracy: 0.9053999781608582\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cluster_batch_normalization  (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "cluster_conv2d (ClusterWeigh (None, 28, 28, 64)        1674      \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d (Clust (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout (ClusterWeig (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_1 (ClusterWei (None, 14, 14, 128)       204938    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_1 (Clu (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_1 (ClusterWe (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "cluster_batch_normalization_ (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "cluster_conv2d_2 (ClusterWei (None, 7, 7, 256)         819466    \n",
            "_________________________________________________________________\n",
            "cluster_max_pooling2d_2 (Clu (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_2 (ClusterWe (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "cluster_flatten (ClusterWeig (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "cluster_dense (ClusterWeight (None, 256)               590090    \n",
            "_________________________________________________________________\n",
            "cluster_activation (ClusterW (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dropout_3 (ClusterWe (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "cluster_dense_1 (ClusterWeig (None, 10)                2580      \n",
            "_________________________________________________________________\n",
            "cluster_activation_1 (Cluste (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,619,520\n",
            "Trainable params: 1,150\n",
            "Non-trainable params: 1,618,370\n",
            "_________________________________________________________________\n",
            "None\n",
            "108/108 [==============================] - 7s 61ms/step - loss: 0.1567 - accuracy: 0.9380 - val_loss: 0.1497 - val_accuracy: 0.9380\n",
            "Test elapsed time: 1.751842737197876 seconds\n",
            "Clustered test accuracy: 0.906000018119812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, dropout_layer_call_fn, dropout_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cluster_model.tf/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['4.0K\\tcluster_model.tf/assets', '13M\\tcluster_model.tf/variables', '13M\\tcluster_model.tf']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XoRiFkL6jqT"
      },
      "source": [
        "# b -> baseline\n",
        "# r -> redux\n",
        "# p -> pruning\n",
        "# q -> quantization\n",
        "# c -> clustering\n",
        "\n",
        "models = ['b', 'r', 'p', 'q', 'c', 'qp', 'pq', 'qc', 'cq', 'pc', 'rqp', 'rpq', 'rqc', 'rcq', 'rpc', 'qpc', 'rqpc']\n",
        "scores = [model_accuracy, \n",
        "          redux_accuracy,\n",
        "          pruned_results[0], \n",
        "          quantized_results[0], \n",
        "          clustered_results[0], \n",
        "          qp_results[0],\n",
        "          pq_results[0],\n",
        "          qc_results[0],\n",
        "          cq_results[0],\n",
        "          pc_results[0],\n",
        "          rqp_results[0],\n",
        "          rpq_results[0],\n",
        "          rqc_results[0],\n",
        "          rcq_results[0],\n",
        "          rpc_results[0],\n",
        "          qpc_results[0],\n",
        "          rqpc_results[0]]\n",
        "times = [baseline_time,\n",
        "         redux_time,\n",
        "         pruned_results[1], \n",
        "         quantized_results[1],\n",
        "         clustered_results[1], \n",
        "         qp_results[1],\n",
        "          pq_results[1],\n",
        "          qc_results[1],\n",
        "          cq_results[1],\n",
        "          pc_results[1],\n",
        "         rqp_results[1],\n",
        "          rpq_results[1],\n",
        "          rqc_results[1],\n",
        "          rcq_results[1],\n",
        "          rpc_results[1],\n",
        "          qpc_results[1],\n",
        "          rqpc_results[1]]\n",
        "dimensions = [baseline_dim, \n",
        "              redux_dim,\n",
        "              pruned_results[2], \n",
        "              quantized_results[2], \n",
        "              clustered_results[2], \n",
        "              qp_results[2],\n",
        "              pq_results[2],\n",
        "              qc_results[2],\n",
        "              cq_results[2],\n",
        "              pc_results[2],\n",
        "              rqp_results[2],\n",
        "              rpq_results[2],\n",
        "              rqc_results[2],\n",
        "              rcq_results[2],\n",
        "              rpc_results[2],\n",
        "              qpc_results[2],\n",
        "              rqpc_results[2]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "id": "YvcX6cKklAEW",
        "outputId": "15f5ffac-ec39-4861-e584-cc353189123b"
      },
      "source": [
        "#Plot all configurations for tradeoff\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Score')\n",
        "plt.plot(models, scores)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Time')\n",
        "plt.plot(models, times)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Dimension')\n",
        "plt.plot(models, dimensions)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--ecO7rWl8SV",
        "outputId": "39966ec1-36a4-46ae-eef0-c6d2af72cf03"
      },
      "source": [
        "# Percentage gain in performance with respect to baseline\n",
        "def get_perc_gain(final_results):\n",
        "  base_acc = scores[0]\n",
        "  base_time = times[0]\n",
        "  base_dim = dimensions[0]\n",
        "\n",
        "  perc_acc_gain = -100 * (1 - final_results[0] / base_acc)\n",
        "  perc_time_gain = -100 * (1 - final_results[1] / base_time)\n",
        "  perc_dim_gain = -100 * (1 - final_results[2] / base_dim)\n",
        "\n",
        "  '''\n",
        "  print('Opt model accuracy: ' + str(final_results[0]))\n",
        "  print('Baseline accuracy: ' + str(base_acc))\n",
        "  print('Opt model time: ' + str(final_results[1]))\n",
        "  print('Baseline time: ' + str(base_time))\n",
        "  print('Opt model dimension: ' + str(final_results[2]))\n",
        "  print('Baseline dimension: ' + str(base_dim))\n",
        "  print('\\n')\n",
        "  '''\n",
        "\n",
        "  print('Accuracy gain flat: ' + str(round(final_results[0] - base_acc, 3)) + ' %' )\n",
        "  print('Accuracy gain percentage: ' + str(round(perc_acc_gain, 2)) + ' %' )\n",
        "\n",
        "  print('Time gain flat: ' + str(round(final_results[1] - base_time, 2)) + ' s' )\n",
        "  print('Time gain percentage: ' + str(round(perc_time_gain, 2)) + ' %')\n",
        "\n",
        "  print('Dimension gain flat: ' + str(final_results[2] - base_dim) + ' Mb' )\n",
        "  print('Dimension gain percentage: ' + str(round(perc_dim_gain, 2)) + ' %')\n",
        "  print('\\n')\n",
        "\n",
        "print('rqp model')\n",
        "get_perc_gain(rqp_results)\n",
        "print('rpq model')\n",
        "get_perc_gain(rpq_results)\n",
        "print('q model')\n",
        "get_perc_gain(quantized_results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rqp model\n",
            "Accuracy gain flat: -0.001 %\n",
            "Accuracy gain percentage: -0.1 %\n",
            "Time gain flat: -0.74 s\n",
            "Time gain percentage: -51.48 %\n",
            "Dimension gain flat: -14.0 Mb\n",
            "Dimension gain percentage: -73.68 %\n",
            "\n",
            "\n",
            "rpq model\n",
            "Accuracy gain flat: 0.003 %\n",
            "Accuracy gain percentage: 0.34 %\n",
            "Time gain flat: -0.13 s\n",
            "Time gain percentage: -9.26 %\n",
            "Dimension gain flat: -12.3 Mb\n",
            "Dimension gain percentage: -64.74 %\n",
            "\n",
            "\n",
            "q model\n",
            "Accuracy gain flat: -0.001 %\n",
            "Accuracy gain percentage: -0.08 %\n",
            "Time gain flat: -0.51 s\n",
            "Time gain percentage: -35.76 %\n",
            "Dimension gain flat: -12.3 Mb\n",
            "Dimension gain percentage: -64.74 %\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQxaE52Y94JH",
        "outputId": "ea640752-57e0-40d4-aa52-afff7c7915de"
      },
      "source": [
        "# number of params gain\n",
        "model = tf.keras.models.load_model(\"./fashion_mnist_model.tf\") \n",
        "param_gain = rqp_model.count_params() - model.count_params()\n",
        "perc_param_gain = str(round(-100 * (1 - rqp_model.count_params() / model.count_params()), 2))\n",
        "\n",
        "print('Number of parameters flat gain: ' + str(param_gain))\n",
        "print('Number of parameters perc gain: ' + str(perc_param_gain) + '%')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters flat gain: -1213184\n",
            "Number of parameters perc gain: -74.91%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AmAkGP1-I8k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvKrG1FPb09e"
      },
      "source": [
        "**CHOICE** <br>\n",
        "The best solutions are:\n",
        "- q: we drop it because it does not reduce the number of parameters\n",
        "- rqp: best performance for Time and Dim but Accuracy is (slightly) lower than rpq\n",
        "- rpq: less efficient in terms of Time and Dim when compared to rqp but higher Accuracy\n",
        "\n",
        "We choose **rqp** model as it has the overall best performance in Time and Dimension while having a relatively small amount of accuracy loss(<0.01%) when compared to rpq. We choose to give more emphasis to the Time parameter as we run the model on a mobile.\n",
        "\n",
        "The final (optimized) model performace when compared to baseline (for percentages look at previous cells):\n",
        "- Accuracy gain flat: -0.001 %\n",
        "- Time gain flat: -0.51 s\n",
        "- Dimension gain flat: -12.3 Mb\n",
        "- Parameter number gain flat:  -1,213,184"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWhfsUACoTTu"
      },
      "source": [
        "# Save the chosen model - rqp\n",
        "rqp_model.save('final_model.tf')\n",
        "final_model = tf.keras.models.load_model(\"./final_model.tf\")\n",
        "final_model.save('final_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7FhEyjniPKs",
        "outputId": "29827083-f354-44bc-da5c-70755e4a2b44"
      },
      "source": [
        "final_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_9 (Batch (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 14, 14, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               147584    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 406,286\n",
            "Trainable params: 406,092\n",
            "Non-trainable params: 194\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chrZ2MlqbeGn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "43d2f2c0-d9bd-41ab-ebea-6c658d15f916"
      },
      "source": [
        "#Plot the tradeoff as percentage gain when compared to baseline\n",
        "'''\n",
        "base_acc = scores[0]\n",
        "base_time = times[0]\n",
        "base_dim = dimensions[0]\n",
        "for i, v in enumerate(scores):\n",
        "  scores[i] = 1 - float(scores[i]/base_acc)\n",
        "  times[i] = 1 - float(times[i]/base_time)\n",
        "  dimensions[i] = 1 - float(dimensions[i]/base_dim)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nbase_acc = scores[0]\\nbase_time = times[0]\\nbase_dim = dimensions[0]\\nfor i, v in enumerate(scores):\\n  scores[i] = 1 - float(scores[i]/base_acc)\\n  times[i] = 1 - float(times[i]/base_time)\\n  dimensions[i] = 1 - float(dimensions[i]/base_dim)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKpph2JwflRj"
      },
      "source": [
        "**PHASE 4 - COREML**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLlb0pfgqYS4",
        "outputId": "28c71317-64c9-4ba2-eeaa-b903f8f3bea7"
      },
      "source": [
        "!pip install coremltools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: coremltools in /usr/local/lib/python3.7/dist-packages (4.1)\n",
            "Requirement already satisfied: numpy<1.20,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from coremltools) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from coremltools) (1.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from coremltools) (1.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from coremltools) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from coremltools) (20.9)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from coremltools) (3.12.4)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from coremltools) (21.2.0)\n",
            "Requirement already satisfied: attr in /usr/local/lib/python3.7/dist-packages (from coremltools) (0.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from coremltools) (4.41.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->coremltools) (1.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->coremltools) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.1.0->coremltools) (56.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5_WGK2HnXoH"
      },
      "source": [
        "import coremltools as ct "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QImFEeGgua4",
        "outputId": "8ac456e7-2077-434e-d3d2-afb4a33f907e"
      },
      "source": [
        "# Convert TF model to CoreML\n",
        "cml_model = ct.convert('final_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running TensorFlow Graph Passes: 100%|██████████| 5/5 [00:00<00:00, 11.12 passes/s]\n",
            "Converting Frontend ==> MIL Ops: 100%|██████████| 47/47 [00:00<00:00, 1373.49 ops/s]\n",
            "Running MIL optimization passes: 100%|██████████| 18/18 [00:00<00:00, 400.76 passes/s]\n",
            "Translating MIL ==> MLModel Ops: 100%|██████████| 79/79 [00:00<00:00, 1451.06 ops/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPJnejfCeCW4"
      },
      "source": [
        "# Change input and output names\n",
        "spec = cml_model.get_spec()\n",
        "\n",
        "ct.utils.rename_feature(spec, 'batch_normalization_9_input', 'input_image')\n",
        "ct.utils.rename_feature(spec, 'Identity', 'output')\n",
        "cml_model = ct.models.MLModel(spec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC3bxW9iiHIU"
      },
      "source": [
        "# Save final model\n",
        "cml_model.save(\"final_model_cml.mlmodel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laQuobRqid6N",
        "outputId": "e30ab9c7-40ec-4fc4-9566-f7efbe6831dd"
      },
      "source": [
        "cml_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "input {\n",
              "  name: \"input_image\"\n",
              "  type {\n",
              "    multiArrayType {\n",
              "      shape: 1\n",
              "      shape: 28\n",
              "      shape: 28\n",
              "      shape: 1\n",
              "      dataType: FLOAT32\n",
              "      shapeRange {\n",
              "        sizeRanges {\n",
              "          lowerBound: 1\n",
              "          upperBound: -1\n",
              "        }\n",
              "        sizeRanges {\n",
              "          lowerBound: 28\n",
              "          upperBound: 28\n",
              "        }\n",
              "        sizeRanges {\n",
              "          lowerBound: 28\n",
              "          upperBound: 28\n",
              "        }\n",
              "        sizeRanges {\n",
              "          lowerBound: 1\n",
              "          upperBound: 1\n",
              "        }\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "}\n",
              "output {\n",
              "  name: \"output\"\n",
              "  type {\n",
              "    multiArrayType {\n",
              "      dataType: FLOAT32\n",
              "    }\n",
              "  }\n",
              "}\n",
              "metadata {\n",
              "  userDefined {\n",
              "    key: \"com.github.apple.coremltools.source\"\n",
              "    value: \"tensorflow==2.5.0\"\n",
              "  }\n",
              "  userDefined {\n",
              "    key: \"com.github.apple.coremltools.version\"\n",
              "    value: \"4.1\"\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "HPwKGTq7_l0A",
        "outputId": "d2f696c6-95d0-433c-912a-ef9c8f5384c0"
      },
      "source": [
        "'''\n",
        "# Define the input type as image, \n",
        "# set pre-processing parameters to normalize the image \n",
        "# to have its values in the interval [-1,1] \n",
        "# as expected by the mobilenet model\n",
        "image_input = ct.ImageType(shape=(1, 28, 28, 1,),\n",
        "                           bias=[-1,-1,-1], scale=1/127)\n",
        "\n",
        "# set class labels\n",
        "classifier_config = ct.ClassifierConfig(class_labels)\n",
        "\n",
        "# Convert the model using the Unified Conversion API\n",
        "model = ct.convert(\n",
        "    final_model, inputs=[image_input], classifier_config=classifier_config,\n",
        ")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Define the input type as image, \\n# set pre-processing parameters to normalize the image \\n# to have its values in the interval [-1,1] \\n# as expected by the mobilenet model\\nimage_input = ct.ImageType(shape=(1, 28, 28, 1,),\\n                           bias=[-1,-1,-1], scale=1/127)\\n\\n# set class labels\\nclassifier_config = ct.ClassifierConfig(class_labels)\\n\\n# Convert the model using the Unified Conversion API\\nmodel = ct.convert(\\n    final_model, inputs=[image_input], classifier_config=classifier_config,\\n)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}
